{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ8t7UpY-cZN",
        "outputId": "d996e350-d718-4bbd-d074-7d7e6c53f61f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "!fusermount -u /content/drive 2>/dev/null\n",
        "!rm -rf /content/drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5HJvY_h-lTf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_bRph9yGl0D",
        "outputId": "4477c96a-ae3f-4c89-e8f4-91be9f39d2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " All generated data will be saved permanently at: /content/drive/MyDrive/wifi_dataset\n",
            "Starting generation of 300 Wi-Fi spectrograms...\n",
            " Resuming from sample 71...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 10/230 [00:23<17:43,  4.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 20/230 [00:45<18:14,  5.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 30/230 [01:04<15:24,  4.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 40/230 [01:24<14:50,  4.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 50/230 [01:47<16:36,  5.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 60/230 [02:10<15:09,  5.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 70/230 [02:30<13:16,  4.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 80/230 [02:49<11:12,  4.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 90/230 [03:09<10:58,  4.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 100/230 [03:28<09:07,  4.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 110/230 [03:47<08:46,  4.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 120/230 [04:05<07:40,  4.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 130/230 [04:23<06:58,  4.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 140/230 [04:41<06:00,  4.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 150/230 [05:01<06:04,  4.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|██████▉   | 160/230 [05:18<04:47,  4.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 170/230 [05:39<04:47,  4.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 180/230 [05:56<03:23,  4.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 190/230 [06:16<03:03,  4.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 200/230 [06:36<02:25,  4.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████▏| 210/230 [07:01<02:00,  6.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 220/230 [07:21<00:45,  4.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 230/230 [07:39<00:00,  2.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            " Dataset generation complete.\n",
            "Spectrograms saved permanently to: /content/drive/MyDrive/wifi_dataset/npz\n",
            "Preview images saved to: /content/drive/MyDrive/wifi_dataset/images\n",
            "Checkpoint file: /content/drive/MyDrive/wifi_dataset/checkpoint.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Colab + Google Drive SAFE Wi-Fi Spectrogram Dataset Generator\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.signal import spectrogram\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- MOUNT DRIVE ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- SETUP OUTPUT PATH ---\n",
        "drive_dir = \"/content/drive/MyDrive/wifi_dataset\"\n",
        "os.makedirs(drive_dir, exist_ok=True)\n",
        "img_dir = os.path.join(drive_dir, \"images\")\n",
        "npz_dir = os.path.join(drive_dir, \"npz\")\n",
        "os.makedirs(img_dir, exist_ok=True)\n",
        "os.makedirs(npz_dir, exist_ok=True)\n",
        "\n",
        "print(f\" All generated data will be saved permanently at: {drive_dir}\")\n",
        "\n",
        "# --- WiFi OFDM Parameters (802.11a/g, 20 MHz channel) ---\n",
        "N_FFT = 64\n",
        "N_CP = 16\n",
        "SC_INDICES = list(range(6, 32)) + list(range(33, 59))\n",
        "N_DATA_SC = len(SC_INDICES)\n",
        "SYMBOL_SAMPLES = N_FFT + N_CP  # 80 samples\n",
        "\n",
        "def create_ofdm_symbol():\n",
        "    \"\"\"Generates a single time-domain 20 MHz OFDM symbol.\"\"\"\n",
        "    symbol_freq_centered = np.zeros(N_FFT, dtype=complex)\n",
        "    real_data = np.random.choice([-1, 1], N_DATA_SC)\n",
        "    imag_data = np.random.choice([-1, 1], N_DATA_SC)\n",
        "    qpsk_data = (real_data + 1j * imag_data) / np.sqrt(2)\n",
        "    symbol_freq_centered[SC_INDICES] = qpsk_data\n",
        "    symbol_freq_ifft = np.fft.ifftshift(symbol_freq_centered)\n",
        "    symbol_time = np.fft.ifft(symbol_freq_ifft)\n",
        "    cp = symbol_time[-N_CP:]\n",
        "    return np.concatenate([cp, symbol_time])\n",
        "\n",
        "def generate_wifi_packet(num_symbols):\n",
        "    \"\"\"Generates a single WiFi packet of N symbols.\"\"\"\n",
        "    return np.concatenate([create_ofdm_symbol() for _ in range(num_symbols)])\n",
        "\n",
        "# --- Simulation Parameters ---\n",
        "fs = 20e6  # 20 MHz\n",
        "snr_db = 10\n",
        "sim_time = 0.05  # Shorter window = faster (50 ms)\n",
        "center_frequency = 2437e6  # MHz\n",
        "\n",
        "# --- Output Count ---\n",
        "NUM_SAMPLES = 300  #  Adjust to 200–300 now; client can regenerate full later\n",
        "print(f\"Starting generation of {NUM_SAMPLES} Wi-Fi spectrograms...\")\n",
        "\n",
        "# --- Precompute master packet and signal power ---\n",
        "max_packet_samples = int(0.0025 * fs)\n",
        "max_symbols_needed = int(np.ceil(max_packet_samples / SYMBOL_SAMPLES))\n",
        "master_packet = generate_wifi_packet(max_symbols_needed)\n",
        "signal_power = np.mean(np.abs(master_packet)**2)\n",
        "\n",
        "# --- Checkpoint file for safe resume ---\n",
        "checkpoint_file = os.path.join(drive_dir, \"checkpoint.txt\")\n",
        "start_idx = 1\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, \"r\") as f:\n",
        "        start_idx = int(f.read().strip()) + 1\n",
        "        print(f\" Resuming from sample {start_idx}...\")\n",
        "\n",
        "# --- Main Loop ---\n",
        "for run_idx in tqdm(range(start_idx, NUM_SAMPLES + 1)):\n",
        "    total_samples = int(sim_time * fs)\n",
        "    iq_clean = np.zeros(total_samples, dtype=complex)\n",
        "\n",
        "    # Random packet length and start time\n",
        "    beacon_duration = np.random.uniform(0.0015, 0.0025)  # 1.5–2.5 ms\n",
        "    packet_samples = int(beacon_duration * fs)\n",
        "    start_time = np.random.uniform(0, sim_time - beacon_duration)\n",
        "    start_sample = int(start_time * fs)\n",
        "\n",
        "    # Insert packet\n",
        "    iq_clean[start_sample:start_sample + packet_samples] = master_packet[:packet_samples]\n",
        "\n",
        "    # Add noise (SNR = 10 dB)\n",
        "    snr_linear = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr_linear\n",
        "    noise_std = np.sqrt(noise_power / 2.0)\n",
        "    noise = noise_std * (np.random.randn(total_samples) + 1j * np.random.randn(total_samples))\n",
        "    iq_noisy = iq_clean + noise\n",
        "\n",
        "    # Spectrogram\n",
        "    f, t, Sxx = spectrogram(\n",
        "        iq_noisy, fs=fs, nperseg=512, noverlap=256,\n",
        "        return_onesided=False, detrend=False, scaling='density'\n",
        "    )\n",
        "    Sxx_dB = 10 * np.log10(np.abs(Sxx) + 1e-12)\n",
        "    Sxx_dB_shifted = np.fft.fftshift(Sxx_dB, axes=0)\n",
        "\n",
        "    # Save compactly to Drive (float32)\n",
        "    npz_path = os.path.join(npz_dir, f\"wifi_random_beacon_{run_idx:04d}.npz\")\n",
        "    np.savez_compressed(npz_path, spectrogram_data=Sxx_dB_shifted.astype(np.float32))\n",
        "\n",
        "    # Optional: save simple grayscale preview (for sanity check)\n",
        "    if run_idx <= 5:\n",
        "        plt.imsave(os.path.join(img_dir, f\"wifi_random_beacon_{run_idx:04d}.png\"),\n",
        "                   Sxx_dB_shifted, cmap='inferno')\n",
        "\n",
        "    # Update checkpoint every 10 samples\n",
        "    if run_idx % 10 == 0:\n",
        "        with open(checkpoint_file, \"w\") as f:\n",
        "            f.write(str(run_idx))\n",
        "        drive.flush_and_unmount()  # make sure data is written to Drive\n",
        "        drive.mount('/content/drive')  # re-mount immediately\n",
        "\n",
        "print(\" Dataset generation complete.\")\n",
        "print(f\"Spectrograms saved permanently to: {npz_dir}\")\n",
        "print(f\"Preview images saved to: {img_dir}\")\n",
        "print(f\"Checkpoint file: {checkpoint_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SgnA37f-n8Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7Y7aOFS-xzA",
        "outputId": "abf30a7b-151e-41dd-fe70-ed60205257b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " All Bluetooth spectrograms will be saved to: /content/drive/MyDrive/bluetooth_dataset/npz\n",
            "Simulating hops across 79 Bluetooth Classic channels (2402–2480 MHz).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 10/300 [00:19<22:01,  4.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 20/300 [00:40<21:43,  4.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 30/300 [00:58<18:12,  4.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 40/300 [01:17<19:01,  4.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 50/300 [01:35<16:50,  4.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 60/300 [01:56<19:34,  4.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 70/300 [02:19<20:47,  5.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 80/300 [02:38<16:45,  4.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 90/300 [03:00<17:33,  5.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 100/300 [03:21<16:12,  4.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 110/300 [03:41<14:19,  4.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 120/300 [04:02<14:35,  4.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 130/300 [04:24<14:18,  5.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 140/300 [04:43<12:06,  4.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 150/300 [05:05<12:33,  5.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 160/300 [05:26<10:46,  4.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 170/300 [05:44<08:54,  4.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 180/300 [06:03<08:29,  4.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 190/300 [06:23<08:25,  4.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 200/300 [06:43<07:20,  4.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 210/300 [07:03<07:05,  4.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 220/300 [07:22<05:55,  4.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 230/300 [09:40<46:32, 39.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 240/300 [11:59<41:07, 41.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 250/300 [14:17<34:05, 40.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 260/300 [16:35<27:24, 41.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 270/300 [18:53<20:26, 40.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 280/300 [21:11<13:37, 40.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 290/300 [23:29<06:49, 40.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [25:47<00:00,  5.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            " Bluetooth spectrogram generation complete.\n",
            "Saved 300 files to: /content/drive/MyDrive/bluetooth_dataset/npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "#  Colab + Drive SAFE Bluetooth (GFSK Hopping) Spectrogram Generator\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from scipy.signal import spectrogram, windows\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- MOUNT DRIVE ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- SAVE PATHS ---\n",
        "drive_dir = \"/content/drive/MyDrive/bluetooth_dataset\"\n",
        "npz_dir = os.path.join(drive_dir, \"npz\")\n",
        "os.makedirs(npz_dir, exist_ok=True)\n",
        "\n",
        "print(f\" All Bluetooth spectrograms will be saved to: {npz_dir}\")\n",
        "\n",
        "# --- Simulation Parameters ---\n",
        "fs = 20e6          # 20 MHz sample rate\n",
        "snr_db = 10\n",
        "sim_time = 0.05     # shorter (50 ms) for faster runtime in Colab\n",
        "center_frequency = 2437e6  # 2.437 GHz (Wi-Fi mid-band)\n",
        "total_samples = int(sim_time * fs)\n",
        "\n",
        "# --- Bluetooth Parameters ---\n",
        "symbol_rate = 1e6                     # 1 Msym/s\n",
        "samples_per_symbol = int(fs / symbol_rate)  # 20\n",
        "hop_rate = 1600                       # 1600 hops/s\n",
        "hop_duration_sec = 1.0 / hop_rate     # 625 microseconds\n",
        "hop_duration_samples = int(hop_duration_sec * fs)  # 12,500 samples\n",
        "symbols_per_hop = int(hop_duration_sec * symbol_rate)  # 625 symbols\n",
        "\n",
        "bt_channels = np.arange(2402e6, 2481e6, 1e6)\n",
        "print(f\"Simulating hops across {len(bt_channels)} Bluetooth Classic channels (2402–2480 MHz).\")\n",
        "\n",
        "# --- Output Config ---\n",
        "NUM_SAMPLES = 300   #  Adjust (200–300 for demo, client can regen full 5000 later)\n",
        "checkpoint_file = os.path.join(drive_dir, \"bt_checkpoint.txt\")\n",
        "start_idx = 1\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, \"r\") as f:\n",
        "        start_idx = int(f.read().strip()) + 1\n",
        "        print(f\" Resuming from sample {start_idx}...\")\n",
        "\n",
        "# --- GFSK Modulation ---\n",
        "def gfsk_modulate(num_symbols, samples_per_symbol, modulation_index=0.32, bt=0.5):\n",
        "    bits = np.random.randint(0, 2, num_symbols)\n",
        "    nrz = bits * 2 - 1\n",
        "    x_rect = np.repeat(nrz, samples_per_symbol)\n",
        "\n",
        "    gauss_len_symbols = 4\n",
        "    gauss_len_samples = gauss_len_symbols * samples_per_symbol\n",
        "    if gauss_len_samples % 2 == 0:\n",
        "        gauss_len_samples += 1\n",
        "\n",
        "    std_samples = 0.35 * samples_per_symbol\n",
        "    gauss_window = windows.gaussian(gauss_len_samples, std=std_samples)\n",
        "    gauss_window /= np.sum(gauss_window)\n",
        "\n",
        "    x_filtered = np.convolve(x_rect, gauss_window, mode='same')\n",
        "    phase_step = (np.pi * modulation_index) / samples_per_symbol\n",
        "    phase = np.cumsum(x_filtered * phase_step)\n",
        "\n",
        "    iq_signal = np.exp(1j * phase)\n",
        "    return iq_signal\n",
        "\n",
        "# --- Main Loop ---\n",
        "for run_idx in tqdm(range(start_idx, NUM_SAMPLES + 1)):\n",
        "    iq_clean = np.zeros(total_samples, dtype=complex)\n",
        "    current_sample = 0\n",
        "    signal_power_acc = 0\n",
        "    num_visible_hops = 0\n",
        "\n",
        "    total_hop_slots = int(np.floor(total_samples / hop_duration_samples))\n",
        "\n",
        "    for _ in range(total_hop_slots):\n",
        "        hop_freq_abs = np.random.choice(bt_channels)\n",
        "        hop_freq_rel = hop_freq_abs - center_frequency\n",
        "\n",
        "        # Only simulate if hop falls inside our 20 MHz observation window\n",
        "        if np.abs(hop_freq_rel) < (fs / 2):\n",
        "            gfsk_baseband = gfsk_modulate(symbols_per_hop, samples_per_symbol)\n",
        "            t_hop = np.arange(hop_duration_samples) / fs\n",
        "            carrier = np.exp(1j * 2 * np.pi * hop_freq_rel * t_hop)\n",
        "            hop_signal = gfsk_baseband * carrier\n",
        "\n",
        "            iq_clean[current_sample:current_sample + hop_duration_samples] = hop_signal\n",
        "            signal_power_acc += np.mean(np.abs(hop_signal)**2)\n",
        "            num_visible_hops += 1\n",
        "\n",
        "        current_sample += hop_duration_samples\n",
        "\n",
        "    # --- Add noise ---\n",
        "    if num_visible_hops > 0:\n",
        "        signal_power = signal_power_acc / num_visible_hops\n",
        "    else:\n",
        "        signal_power = 1e-9\n",
        "\n",
        "    snr_linear = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr_linear\n",
        "    noise_std = np.sqrt(noise_power / 2.0)\n",
        "    noise = noise_std * (np.random.randn(total_samples) + 1j * np.random.randn(total_samples))\n",
        "    iq_noisy = iq_clean + noise\n",
        "\n",
        "    # --- Spectrogram ---\n",
        "    f, t, Sxx = spectrogram(iq_noisy, fs=fs, nperseg=512, noverlap=256, return_onesided=False)\n",
        "    Sxx_dB = 10 * np.log10(np.abs(Sxx) + 1e-12)\n",
        "    Sxx_dB_shifted = np.fft.fftshift(Sxx_dB, axes=0).astype(np.float32)\n",
        "\n",
        "    # --- Save to Drive ---\n",
        "    npz_path = os.path.join(npz_dir, f\"bt_spectrogram_{run_idx:04d}.npz\")\n",
        "    np.savez_compressed(npz_path, spectrogram_data=Sxx_dB_shifted)\n",
        "\n",
        "    # --- Save checkpoint every 10 runs ---\n",
        "    if run_idx % 10 == 0:\n",
        "        with open(checkpoint_file, \"w\") as f:\n",
        "            f.write(str(run_idx))\n",
        "        drive.flush_and_unmount()\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "print(\" Bluetooth spectrogram generation complete.\")\n",
        "print(f\"Saved {NUM_SAMPLES} files to: {npz_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewh7nR5r-q_P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK1jkeyr_PF2",
        "outputId": "4b5f6ca1-2c66-4e3d-846d-b0160ab9070d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " ZigBee spectrograms will be saved in: /content/drive/MyDrive/zigbee_dataset/npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 10/300 [02:18<3:19:39, 41.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 20/300 [04:37<3:11:58, 41.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 30/300 [06:55<3:03:49, 40.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 40/300 [09:13<2:58:10, 41.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 50/300 [11:31<2:50:53, 41.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 60/300 [13:50<2:44:57, 41.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 70/300 [16:08<2:37:22, 41.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 80/300 [18:24<2:28:38, 40.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 90/300 [20:42<2:24:10, 41.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 100/300 [23:00<2:16:48, 41.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 110/300 [25:19<2:10:40, 41.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 120/300 [27:39<2:04:59, 41.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 130/300 [29:56<1:55:49, 40.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 140/300 [32:14<1:49:05, 40.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 150/300 [34:33<1:42:54, 41.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 160/300 [36:51<1:35:17, 40.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 170/300 [39:09<1:28:48, 40.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 180/300 [41:27<1:21:58, 40.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 190/300 [43:46<1:15:41, 41.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 200/300 [46:04<1:08:32, 41.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 210/300 [48:20<1:00:51, 40.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 220/300 [50:40<55:37, 41.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 230/300 [52:56<47:09, 40.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 240/300 [55:12<40:32, 40.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 250/300 [57:28<33:41, 40.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 260/300 [59:44<27:01, 40.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 270/300 [1:02:02<20:32, 41.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 280/300 [1:04:18<13:34, 40.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 290/300 [1:06:34<06:44, 40.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [1:08:50<00:00, 13.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            " ZigBee dataset generation complete.\n",
            "Saved 300 files in: /content/drive/MyDrive/zigbee_dataset/npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Colab + Drive Safe ZigBee Spectrogram Dataset Generator\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from scipy.signal import spectrogram, windows\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- MOUNT DRIVE ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- OUTPUT PATHS ---\n",
        "drive_dir = \"/content/drive/MyDrive/zigbee_dataset\"\n",
        "npz_dir = os.path.join(drive_dir, \"npz\")\n",
        "os.makedirs(npz_dir, exist_ok=True)\n",
        "print(f\" ZigBee spectrograms will be saved in: {npz_dir}\")\n",
        "\n",
        "# ============================================================\n",
        "# --- ZigBee DSSS spreading sequences (IEEE 802.15.4) ---\n",
        "# ============================================================\n",
        "DSSS_TABLE = {\n",
        "    0x0: np.array([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
        "                   1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0]),\n",
        "    0x1: np.array([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
        "                   1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1]),\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# --- Helper Functions ---\n",
        "# ============================================================\n",
        "def zigbee_signal(num_symbols=100):\n",
        "    chips = []\n",
        "    for _ in range(num_symbols):\n",
        "        nibble = np.random.choice(list(DSSS_TABLE.keys()))\n",
        "        chips.extend(DSSS_TABLE[nibble])\n",
        "    return np.array(chips)\n",
        "\n",
        "def oqpsk_modulate(chips, samples_per_chip=10):\n",
        "    chips_pm = chips * 2 - 1\n",
        "    i_chips = chips_pm[0::2]\n",
        "    q_chips = chips_pm[1::2]\n",
        "    i_signal = np.repeat(i_chips, samples_per_chip)\n",
        "    q_signal = np.repeat(q_chips, samples_per_chip)\n",
        "    q_signal = np.roll(q_signal, samples_per_chip // 2)\n",
        "    return i_signal + 1j * q_signal\n",
        "\n",
        "# ============================================================\n",
        "# --- Simulation Parameters ---\n",
        "# ============================================================\n",
        "fs = 20e6\n",
        "snr_db = 10\n",
        "sim_time = 0.05  # shorter for Colab efficiency\n",
        "center_frequency = 2437e6\n",
        "chip_rate = 2e6\n",
        "samples_per_chip = int(fs / chip_rate)\n",
        "beacon_duration = 0.004064  # ~4 ms ZigBee frame\n",
        "\n",
        "# ============================================================\n",
        "# --- Configurable number of samples ---\n",
        "# ============================================================\n",
        "NUM_SAMPLES = 300  #  Adjust for demo (client can regenerate full 5000)\n",
        "checkpoint_file = os.path.join(drive_dir, \"zigbee_checkpoint.txt\")\n",
        "start_idx = 1\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, \"r\") as f:\n",
        "        start_idx = int(f.read().strip()) + 1\n",
        "        print(f\" Resuming from sample {start_idx}...\")\n",
        "\n",
        "# ============================================================\n",
        "# --- Main Simulation Loop ---\n",
        "# ============================================================\n",
        "for run_idx in tqdm(range(start_idx, NUM_SAMPLES + 1)):\n",
        "    total_samples = int(sim_time * fs)\n",
        "    iq_clean = np.zeros(total_samples, dtype=complex)\n",
        "\n",
        "    # --- Generate random ZigBee packet ---\n",
        "    packet_samples = int(beacon_duration * fs)\n",
        "    total_chips_needed = int(np.ceil((packet_samples * 2) / samples_per_chip))\n",
        "    symbols_needed = int(np.ceil(total_chips_needed / 32))\n",
        "    master_chips = zigbee_signal(num_symbols=symbols_needed)\n",
        "    master_packet = oqpsk_modulate(master_chips, samples_per_chip)\n",
        "\n",
        "    signal_power = np.mean(np.abs(master_packet) ** 2)\n",
        "\n",
        "    # --- Random start time ---\n",
        "    latest_start_time = sim_time - beacon_duration\n",
        "    start_time = np.random.uniform(0, latest_start_time)\n",
        "    start_sample = int(start_time * fs)\n",
        "    end_sample = start_sample + packet_samples\n",
        "    if end_sample > total_samples:\n",
        "        end_sample = total_samples\n",
        "        packet_samples = end_sample - start_sample\n",
        "\n",
        "    iq_clean[start_sample:end_sample] = master_packet[:packet_samples]\n",
        "\n",
        "    # --- Add noise ---\n",
        "    snr_linear = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr_linear\n",
        "    noise_std = np.sqrt(noise_power / 2.0)\n",
        "    noise = noise_std * (np.random.randn(total_samples) + 1j * np.random.randn(total_samples))\n",
        "    iq_noisy = iq_clean + noise\n",
        "\n",
        "    # --- Spectrogram ---\n",
        "    f, t, Sxx = spectrogram(iq_noisy, fs=fs, nperseg=512, noverlap=256, return_onesided=False)\n",
        "    Sxx_dB = 10 * np.log10(np.abs(Sxx) + 1e-12)\n",
        "    Sxx_dB_shifted = np.fft.fftshift(Sxx_dB, axes=0).astype(np.float32)\n",
        "\n",
        "    # --- Save matrix ---\n",
        "    npz_path = os.path.join(npz_dir, f\"zigbee_spectrogram_{run_idx:04d}.npz\")\n",
        "    np.savez_compressed(npz_path, spectrogram_data=Sxx_dB_shifted)\n",
        "\n",
        "    # --- Checkpoint every 10 runs ---\n",
        "    if run_idx % 10 == 0:\n",
        "        with open(checkpoint_file, \"w\") as f:\n",
        "            f.write(str(run_idx))\n",
        "        drive.flush_and_unmount()\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "print(\" ZigBee dataset generation complete.\")\n",
        "print(f\"Saved {NUM_SAMPLES} files in: {npz_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AkhcfT8o-yG_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5kUVdLh2A7NG",
        "outputId": "120092e9-faf5-428e-f25f-7f033ab586fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 300 Wi-Fi + ZigBee spectrograms...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:40<00:00,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved 300 files in: /content/drive/MyDrive/wifi_zigbee_dataset/npz\n",
            "Spectrograms saved permanently to: /content/drive/MyDrive/wifi_zigbee_dataset/npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import spectrogram\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# --- Configuration ---\n",
        "# ============================================================\n",
        "fs = 20e6\n",
        "snr_db = 10\n",
        "sim_time = 0.05\n",
        "center_frequency = 2437e6\n",
        "num_samples = int(sim_time * fs)\n",
        "N_SAMPLES = 300  # fixed 300\n",
        "\n",
        "# ============================================================\n",
        "# --- Paths ---\n",
        "# ============================================================\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "dataset_dir = os.path.join(base_path, \"wifi_zigbee_dataset\")\n",
        "npz_dir = os.path.join(dataset_dir, \"npz\")\n",
        "os.makedirs(npz_dir, exist_ok=True)\n",
        "\n",
        "# Skip generation if already complete\n",
        "existing = [f for f in os.listdir(npz_dir) if f.endswith(\".npz\")]\n",
        "if len(existing) >= N_SAMPLES:\n",
        "    print(f\"Dataset already complete with {len(existing)} files in: {npz_dir}\")\n",
        "else:\n",
        "    print(f\"Generating {N_SAMPLES - len(existing)} Wi-Fi + ZigBee spectrograms...\")\n",
        "\n",
        "    # ============================================================\n",
        "    # --- WiFi OFDM Parameters ---\n",
        "    # ============================================================\n",
        "    N_FFT = 64\n",
        "    N_CP = 16\n",
        "    SC_INDICES = list(range(6, 32)) + list(range(33, 59))\n",
        "    N_DATA_SC = len(SC_INDICES)\n",
        "    WIFI_SYMBOL_SAMPLES = N_FFT + N_CP\n",
        "\n",
        "    def create_ofdm_symbol():\n",
        "        symbol_freq_centered = np.zeros(N_FFT, dtype=complex)\n",
        "        re = np.random.choice([-1, 1], N_DATA_SC)\n",
        "        im = np.random.choice([-1, 1], N_DATA_SC)\n",
        "        qpsk_data = (re + 1j * im) / np.sqrt(2)\n",
        "        symbol_freq_centered[SC_INDICES] = qpsk_data\n",
        "        symbol_freq_ifft = np.fft.ifftshift(symbol_freq_centered)\n",
        "        symbol_time = np.fft.ifft(symbol_freq_ifft)\n",
        "        cp = symbol_time[-N_CP:]\n",
        "        return np.concatenate([cp, symbol_time])\n",
        "\n",
        "    def generate_wifi_packet(num_symbols):\n",
        "        return np.concatenate([create_ofdm_symbol() for _ in range(num_symbols)])\n",
        "\n",
        "    # ============================================================\n",
        "    # --- ZigBee DSSS (OQPSK) ---\n",
        "    # ============================================================\n",
        "    DSSS_TABLE = {\n",
        "        0x0: np.array([1,1,1,1,1,0,0,1,1,0,0,0,1,0,0,1,\n",
        "                       1,1,0,0,1,1,0,1,0,1,0,0,1,0,1,0]),\n",
        "        0x1: np.array([0,1,1,1,1,1,0,0,1,1,0,0,0,1,0,0,\n",
        "                       1,1,1,0,0,1,1,0,1,0,1,0,0,1,0,1]),\n",
        "    }\n",
        "\n",
        "    def zigbee_signal(num_symbols=100):\n",
        "        chips = []\n",
        "        for _ in range(num_symbols):\n",
        "            nib = np.random.choice(list(DSSS_TABLE.keys()))\n",
        "            chips.extend(DSSS_TABLE[nib])\n",
        "        return np.array(chips)\n",
        "\n",
        "    def oqpsk_modulate(chips, samples_per_chip=10):\n",
        "        chips_pm = chips * 2 - 1\n",
        "        i_chips = chips_pm[0::2]\n",
        "        q_chips = chips_pm[1::2]\n",
        "        i_sig = np.repeat(i_chips, samples_per_chip)\n",
        "        q_sig = np.repeat(q_chips, samples_per_chip)\n",
        "        q_sig = np.roll(q_sig, samples_per_chip // 2)\n",
        "        return i_sig + 1j * q_sig\n",
        "\n",
        "    # ============================================================\n",
        "    # --- Precompute Packets ---\n",
        "    # ============================================================\n",
        "    wifi_symbols_needed = int(np.ceil((0.0025 * fs) / WIFI_SYMBOL_SAMPLES))\n",
        "    master_wifi_packet = generate_wifi_packet(wifi_symbols_needed)\n",
        "\n",
        "    zigbee_duration = 0.004064\n",
        "    zigbee_samples_per_chip = int(fs / 2e6)\n",
        "    zigbee_chips_needed = int(np.ceil((zigbee_duration * fs * 2) / zigbee_samples_per_chip))\n",
        "    zigbee_symbols_needed = int(np.ceil(zigbee_chips_needed / 32))\n",
        "    master_zb_chips = zigbee_signal(zigbee_symbols_needed)\n",
        "    master_zb_packet = oqpsk_modulate(master_zb_chips, zigbee_samples_per_chip)\n",
        "\n",
        "    wifi_channels = [2412e6, 2437e6, 2462e6]\n",
        "    zigbee_channels = 2405e6 + 5e6 * np.arange(11, 27)\n",
        "\n",
        "    def place_carrier(x, fs, f_sig, f_obs):\n",
        "        t = np.arange(x.size) / fs\n",
        "        return x * np.exp(1j * 2 * np.pi * (f_sig - f_obs) * t)\n",
        "\n",
        "    # ============================================================\n",
        "    # --- Main Generation Loop ---\n",
        "    # ============================================================\n",
        "    for i in tqdm(range(len(existing), N_SAMPLES), total=N_SAMPLES, initial=len(existing)):\n",
        "        iq_clean = np.zeros(num_samples, dtype=complex)\n",
        "\n",
        "        # --- Wi-Fi burst ---\n",
        "        wifi_dur = np.random.uniform(0.0015, 0.0025)\n",
        "        wifi_samples = int(wifi_dur * fs)\n",
        "        wifi_start = np.random.randint(0, num_samples - wifi_samples)\n",
        "        wifi_sig = master_wifi_packet[:wifi_samples]\n",
        "        wifi_scale = 10 ** (np.random.uniform(-5, +5) / 20)\n",
        "        wifi_center = np.random.choice(wifi_channels)\n",
        "        wifi_sig = place_carrier(wifi_sig * wifi_scale, fs, wifi_center, center_frequency)\n",
        "        iq_clean[wifi_start:wifi_start + wifi_samples] += wifi_sig\n",
        "\n",
        "        # --- ZigBee burst ---\n",
        "        zb_samples = int(zigbee_duration * fs)\n",
        "        zb_start = np.random.randint(0, num_samples - zb_samples)\n",
        "        zb_sig = master_zb_packet[:zb_samples]\n",
        "        zb_scale = 10 ** (np.random.uniform(-10, +3) / 20)\n",
        "        zb_center = np.random.choice(zigbee_channels)\n",
        "        zb_sig = place_carrier(zb_sig * zb_scale, fs, zb_center, center_frequency)\n",
        "        iq_clean[zb_start:zb_start + zb_samples] += zb_sig\n",
        "\n",
        "        # --- Add noise ---\n",
        "        total_power = np.mean(np.abs(iq_clean)**2)\n",
        "        snr_linear = 10 ** (snr_db / 10)\n",
        "        noise_power = total_power / snr_linear\n",
        "        noise_std = np.sqrt(noise_power / 2)\n",
        "        noise = noise_std * (np.random.randn(num_samples) + 1j * np.random.randn(num_samples))\n",
        "        iq_noisy = iq_clean + noise\n",
        "\n",
        "        # --- Spectrogram ---\n",
        "        f, t, Sxx = spectrogram(iq_noisy, fs=fs, nperseg=512, noverlap=256, return_onesided=False)\n",
        "        Sxx_dB = 10 * np.log10(np.abs(Sxx) + 1e-12)\n",
        "        Sxx_dB = np.fft.fftshift(Sxx_dB, axes=0).astype(np.float32)\n",
        "\n",
        "        # --- Save ---\n",
        "        np.savez_compressed(\n",
        "            os.path.join(npz_dir, f\"wifi_zigbee_{i:04d}.npz\"),\n",
        "            spectrogram_data=Sxx_dB,\n",
        "            fs=fs,\n",
        "            f_center=center_frequency,\n",
        "            label=\"wifi+zigbee\",\n",
        "            wifi_center=wifi_center,\n",
        "            zigbee_center=zb_center,\n",
        "            wifi_scale=wifi_scale,\n",
        "            zigbee_scale=zb_scale,\n",
        "            snr_db=snr_db\n",
        "        )\n",
        "\n",
        "    print(f\"Saved {N_SAMPLES} files in: {npz_dir}\")\n",
        "\n",
        "print(f\"Spectrograms saved permanently to: {npz_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bfXdRzYHIurW",
        "outputId": "8e12ada1-b56d-4ba2-c13b-213becd957ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 300 Bluetooth + ZigBee spectrograms...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:48<00:00,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved 300 files in: /content/drive/MyDrive/bluetooth_zigbee_dataset/npz\n",
            "Spectrograms saved permanently to: /content/drive/MyDrive/bluetooth_zigbee_dataset/npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import spectrogram, windows\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# --- Configuration ---\n",
        "# ============================================================\n",
        "fs = 20e6\n",
        "snr_db = 10\n",
        "sim_time = 0.05\n",
        "num_samples = int(sim_time * fs)\n",
        "center_frequency = 2437e6\n",
        "snr_linear = 10 ** (snr_db / 10)\n",
        "N_SAMPLES = 300\n",
        "\n",
        "# ============================================================\n",
        "# --- Paths ---\n",
        "# ============================================================\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "dataset_dir = os.path.join(base_path, \"bluetooth_zigbee_dataset\")\n",
        "npz_dir = os.path.join(dataset_dir, \"npz\")\n",
        "os.makedirs(npz_dir, exist_ok=True)\n",
        "\n",
        "# --- Skip generation if already done ---\n",
        "existing = [f for f in os.listdir(npz_dir) if f.endswith(\".npz\")]\n",
        "if len(existing) >= N_SAMPLES:\n",
        "    print(f\"Dataset already complete with {len(existing)} files in: {npz_dir}\")\n",
        "else:\n",
        "    print(f\"Generating {N_SAMPLES - len(existing)} Bluetooth + ZigBee spectrograms...\")\n",
        "\n",
        "    # ============================================================\n",
        "    # --- Bluetooth GFSK ---\n",
        "    # ============================================================\n",
        "    def gfsk_modulate(num_symbols, samples_per_symbol, modulation_index=0.32):\n",
        "        bits = np.random.randint(0, 2, num_symbols)\n",
        "        nrz = bits * 2 - 1\n",
        "        x_rect = np.repeat(nrz, samples_per_symbol)\n",
        "        g_len = 4 * samples_per_symbol + 1\n",
        "        std = 0.35 * samples_per_symbol\n",
        "        g = windows.gaussian(g_len, std=std)\n",
        "        g /= np.sum(g)\n",
        "        x_filtered = np.convolve(x_rect, g, mode='same')\n",
        "        phase_step = (np.pi * modulation_index) / samples_per_symbol\n",
        "        phase = np.cumsum(x_filtered * phase_step)\n",
        "        return np.exp(1j * phase)\n",
        "\n",
        "    bt_symbol_rate = 1e6\n",
        "    bt_samples_per_symbol = int(fs / bt_symbol_rate)\n",
        "    hop_rate = 1600\n",
        "    hop_duration_sec = 1.0 / hop_rate\n",
        "    hop_duration_samples = int(hop_duration_sec * fs)\n",
        "    bt_symbols_per_hop = int(hop_duration_sec * bt_symbol_rate)\n",
        "    bt_channels = np.arange(2402e6, 2481e6, 1e6)\n",
        "    total_hop_slots = int(np.floor(num_samples / hop_duration_samples))\n",
        "\n",
        "    # ============================================================\n",
        "    # --- ZigBee OQPSK ---\n",
        "    # ============================================================\n",
        "    DSSS_TABLE = {\n",
        "        0x0: np.array([1,1,1,1,1,0,0,1,1,0,0,0,1,0,0,1,\n",
        "                       1,1,0,0,1,1,0,1,0,1,0,0,1,0,1,0]),\n",
        "        0x1: np.array([0,1,1,1,1,1,0,0,1,1,0,0,0,1,0,0,\n",
        "                       1,1,1,0,0,1,1,0,1,0,1,0,0,1,0,1]),\n",
        "    }\n",
        "\n",
        "    def zigbee_signal(num_symbols=100):\n",
        "        chips = []\n",
        "        for _ in range(num_symbols):\n",
        "            nibble = np.random.choice(list(DSSS_TABLE.keys()))\n",
        "            chips.extend(DSSS_TABLE[nibble])\n",
        "        return np.array(chips)\n",
        "\n",
        "    def oqpsk_modulate(chips, samples_per_chip=10):\n",
        "        chips_pm = chips * 2 - 1\n",
        "        i_chips = chips_pm[0::2]\n",
        "        q_chips = chips_pm[1::2]\n",
        "        i_signal = np.repeat(i_chips, samples_per_chip)\n",
        "        q_signal = np.repeat(q_chips, samples_per_chip)\n",
        "        q_signal = np.roll(q_signal, samples_per_chip // 2)\n",
        "        return i_signal + 1j * q_signal\n",
        "\n",
        "    zigbee_chip_rate = 2e6\n",
        "    zigbee_samples_per_chip = int(fs / zigbee_chip_rate)\n",
        "    zigbee_duration = 0.004064\n",
        "    zigbee_channels = 2405e6 + 5e6 * np.arange(11, 27)\n",
        "    zigbee_chips_needed = int(np.ceil((zigbee_duration * fs * 2) / zigbee_samples_per_chip))\n",
        "    zigbee_symbols_needed = int(np.ceil(zigbee_chips_needed / 32))\n",
        "    master_zb_chips = zigbee_signal(zigbee_symbols_needed)\n",
        "    master_zb_packet = oqpsk_modulate(master_zb_chips, zigbee_samples_per_chip)[:int(zigbee_duration * fs)]\n",
        "\n",
        "    def place_carrier(x, fs, f_sig, f_obs):\n",
        "        t = np.arange(x.size) / fs\n",
        "        return x * np.exp(1j * 2 * np.pi * (f_sig - f_obs) * t)\n",
        "\n",
        "    # ============================================================\n",
        "    # --- Main Loop ---\n",
        "    # ============================================================\n",
        "    for run_idx in tqdm(range(len(existing), N_SAMPLES), total=N_SAMPLES, initial=len(existing)):\n",
        "        iq_clean = np.zeros(num_samples, dtype=complex)\n",
        "        signal_power_acc = 0\n",
        "        num_visible_hops = 0\n",
        "        cur_sample = 0\n",
        "\n",
        "        # --- Bluetooth Hops ---\n",
        "        for _ in range(total_hop_slots):\n",
        "            f_bt_abs = np.random.choice(bt_channels)\n",
        "            f_bt_rel = f_bt_abs - center_frequency\n",
        "            if np.abs(f_bt_rel) < (fs / 2):\n",
        "                gfsk = gfsk_modulate(bt_symbols_per_hop, bt_samples_per_symbol)\n",
        "                t_hop = np.arange(hop_duration_samples) / fs\n",
        "                carrier = np.exp(1j * 2 * np.pi * f_bt_rel * t_hop)\n",
        "                hop_signal = gfsk * carrier\n",
        "                power_scale_bt = 10 ** (np.random.uniform(-8, +5) / 20)\n",
        "                hop_signal *= power_scale_bt\n",
        "                iq_clean[cur_sample:cur_sample + hop_duration_samples] = hop_signal\n",
        "                signal_power_acc += np.mean(np.abs(hop_signal)**2)\n",
        "                num_visible_hops += 1\n",
        "            cur_sample += hop_duration_samples\n",
        "\n",
        "        # --- ZigBee Burst ---\n",
        "        zb_samples = int(zigbee_duration * fs)\n",
        "        zb_start = np.random.randint(0, num_samples - zb_samples)\n",
        "        zb_sig = master_zb_packet[:zb_samples].copy()\n",
        "        power_scale_zb = 10 ** (np.random.uniform(-10, +3) / 20)\n",
        "        f_zb = np.random.choice(zigbee_channels)\n",
        "        zb_sig = place_carrier(zb_sig * power_scale_zb, fs, f_zb, center_frequency)\n",
        "        iq_clean[zb_start:zb_start + zb_samples] += zb_sig\n",
        "\n",
        "        # --- Add noise ---\n",
        "        total_power = np.mean(np.abs(iq_clean)**2)\n",
        "        noise_power = total_power / snr_linear\n",
        "        noise_std = np.sqrt(noise_power / 2)\n",
        "        noise = noise_std * (np.random.randn(num_samples) + 1j * np.random.randn(num_samples))\n",
        "        iq_noisy = iq_clean + noise\n",
        "\n",
        "        # --- Spectrogram ---\n",
        "        f, t, Sxx = spectrogram(iq_noisy, fs=fs, nperseg=512, noverlap=256, return_onesided=False)\n",
        "        Sxx_dB = 10 * np.log10(np.abs(Sxx) + 1e-12)\n",
        "        Sxx_dB = np.fft.fftshift(Sxx_dB, axes=0).astype(np.float32)\n",
        "\n",
        "        # --- Save ---\n",
        "        np.savez_compressed(\n",
        "            os.path.join(npz_dir, f\"bluetooth_zigbee_{run_idx:04d}.npz\"),\n",
        "            spectrogram_data=Sxx_dB,\n",
        "            fs=fs,\n",
        "            f_center=center_frequency,\n",
        "            label=\"bluetooth+zigbee\",\n",
        "            bt_power_scale=power_scale_bt,\n",
        "            zb_power_scale=power_scale_zb,\n",
        "            bt_visible_hops=num_visible_hops,\n",
        "            f_zigbee=f_zb,\n",
        "            snr_db=snr_db\n",
        "        )\n",
        "\n",
        "    print(f\"Saved {N_SAMPLES} files in: {npz_dir}\")\n",
        "\n",
        "print(f\"Spectrograms saved permanently to: {npz_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JX7OpQoM-1wA",
        "outputId": "bc87de76-6d85-4bd5-ebfd-2511ed0e6d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 300 Wi-Fi + Bluetooth + ZigBee spectrograms...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:52<00:00,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved 300 files in: /content/drive/MyDrive/wifi_bluetooth_zigbee_dataset/npz\n",
            "Spectrograms saved permanently to: /content/drive/MyDrive/wifi_bluetooth_zigbee_dataset/npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import spectrogram, windows\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# --- Configuration ---\n",
        "# ============================================================\n",
        "fs = 20e6\n",
        "snr_db = 10\n",
        "sim_time = 0.05\n",
        "num_samples = int(sim_time * fs)\n",
        "center_frequency = 2437e6\n",
        "snr_linear = 10 ** (snr_db / 10)\n",
        "N_SAMPLES = 300\n",
        "\n",
        "# ============================================================\n",
        "# --- Paths ---\n",
        "# ============================================================\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "dataset_dir = os.path.join(base_path, \"wifi_bluetooth_zigbee_dataset\")\n",
        "npz_dir = os.path.join(dataset_dir, \"npz\")\n",
        "os.makedirs(npz_dir, exist_ok=True)\n",
        "\n",
        "# --- Skip generation if already done ---\n",
        "existing = [f for f in os.listdir(npz_dir) if f.endswith(\".npz\")]\n",
        "if len(existing) >= N_SAMPLES:\n",
        "    print(f\"Dataset already complete with {len(existing)} files in: {npz_dir}\")\n",
        "    print(f\"Spectrograms saved permanently to: {npz_dir}\")\n",
        "    exit()\n",
        "else:\n",
        "    print(f\"Generating {N_SAMPLES - len(existing)} Wi-Fi + Bluetooth + ZigBee spectrograms...\")\n",
        "\n",
        "# ============================================================\n",
        "# --- Wi-Fi (OFDM, 802.11a/g) ---\n",
        "# ============================================================\n",
        "N_FFT, N_CP = 64, 16\n",
        "SC_INDICES = list(range(6, 32)) + list(range(33, 59))\n",
        "N_DATA_SC = len(SC_INDICES)\n",
        "WIFI_SYMBOL_SAMPLES = N_FFT + N_CP\n",
        "wifi_channels = [2412e6, 2437e6, 2462e6]\n",
        "\n",
        "def create_ofdm_symbol():\n",
        "    sym_freq = np.zeros(N_FFT, dtype=complex)\n",
        "    re = np.random.choice([-1, 1], N_DATA_SC)\n",
        "    im = np.random.choice([-1, 1], N_DATA_SC)\n",
        "    qpsk_data = (re + 1j * im) / np.sqrt(2)\n",
        "    sym_freq[SC_INDICES] = qpsk_data\n",
        "    sym_time = np.fft.ifft(np.fft.ifftshift(sym_freq))\n",
        "    return np.concatenate([sym_time[-N_CP:], sym_time])\n",
        "\n",
        "def generate_wifi_packet(num_symbols=100):\n",
        "    return np.concatenate([create_ofdm_symbol() for _ in range(num_symbols)])\n",
        "\n",
        "# ============================================================\n",
        "# --- Bluetooth (GFSK, 1 Msym/s, 1600 hops/s) ---\n",
        "# ============================================================\n",
        "def gfsk_modulate(num_symbols, samples_per_symbol, modulation_index=0.32):\n",
        "    bits = np.random.randint(0, 2, num_symbols)\n",
        "    nrz = bits * 2 - 1\n",
        "    x_rect = np.repeat(nrz, samples_per_symbol)\n",
        "    g_len = 4 * samples_per_symbol + 1\n",
        "    std = 0.35 * samples_per_symbol\n",
        "    g = windows.gaussian(g_len, std=std)\n",
        "    g /= np.sum(g)\n",
        "    x_filt = np.convolve(x_rect, g, mode=\"same\")\n",
        "    phase_step = (np.pi * modulation_index) / samples_per_symbol\n",
        "    phase = np.cumsum(x_filt * phase_step)\n",
        "    return np.exp(1j * phase)\n",
        "\n",
        "bt_symbol_rate = 1e6\n",
        "bt_samples_per_symbol = int(fs / bt_symbol_rate)\n",
        "hop_rate = 1600\n",
        "hop_duration_sec = 1.0 / hop_rate\n",
        "hop_duration_samples = int(hop_duration_sec * fs)\n",
        "bt_symbols_per_hop = int(hop_duration_sec * bt_symbol_rate)\n",
        "bt_channels = np.arange(2402e6, 2481e6, 1e6)\n",
        "total_hop_slots = int(np.floor(num_samples / hop_duration_samples))\n",
        "\n",
        "# ============================================================\n",
        "# --- ZigBee (OQPSK, 2 MHz) ---\n",
        "# ============================================================\n",
        "DSSS_TABLE = {\n",
        "    0x0: np.array([1,1,1,1,1,0,0,1,1,0,0,0,1,0,0,1,\n",
        "                   1,1,0,0,1,1,0,1,0,1,0,0,1,0,1,0]),\n",
        "    0x1: np.array([0,1,1,1,1,1,0,0,1,1,0,0,0,1,0,0,\n",
        "                   1,1,1,0,0,1,1,0,1,0,1,0,0,1,0,1]),\n",
        "}\n",
        "\n",
        "def zigbee_signal(num_symbols=100):\n",
        "    chips = []\n",
        "    for _ in range(num_symbols):\n",
        "        nib = np.random.choice(list(DSSS_TABLE.keys()))\n",
        "        chips.extend(DSSS_TABLE[nib])\n",
        "    return np.array(chips)\n",
        "\n",
        "def oqpsk_modulate(chips, samples_per_chip=10):\n",
        "    chips_pm = chips * 2 - 1\n",
        "    i_chips = chips_pm[0::2]\n",
        "    q_chips = chips_pm[1::2]\n",
        "    i_signal = np.repeat(i_chips, samples_per_chip)\n",
        "    q_signal = np.repeat(q_chips, samples_per_chip)\n",
        "    q_signal = np.roll(q_signal, samples_per_chip // 2)\n",
        "    return i_signal + 1j * q_signal\n",
        "\n",
        "zigbee_chip_rate = 2e6\n",
        "zigbee_samples_per_chip = int(fs / zigbee_chip_rate)\n",
        "zigbee_dur = 0.004064\n",
        "zigbee_channels = 2405e6 + 5e6 * np.arange(11, 27)\n",
        "zigbee_chips_needed = int(np.ceil((zigbee_dur * fs * 2) / zigbee_samples_per_chip))\n",
        "zigbee_symbols_needed = int(np.ceil(zigbee_chips_needed / 32))\n",
        "master_zb_chips = zigbee_signal(zigbee_symbols_needed)\n",
        "master_zb_packet = oqpsk_modulate(master_zb_chips, zigbee_samples_per_chip)[:int(zigbee_dur * fs)]\n",
        "\n",
        "# ============================================================\n",
        "# --- Helper Functions ---\n",
        "# ============================================================\n",
        "def place_carrier(x, fs, f_sig, f_obs):\n",
        "    t = np.arange(x.size) / fs\n",
        "    return x * np.exp(1j * 2 * np.pi * (f_sig - f_obs) * t)\n",
        "\n",
        "def safe_insert(buffer, signal, start_idx):\n",
        "    end_idx = min(start_idx + len(signal), len(buffer))\n",
        "    if end_idx <= start_idx:\n",
        "        return 0\n",
        "    sig_len = end_idx - start_idx\n",
        "    buffer[start_idx:end_idx] += signal[:sig_len]\n",
        "    return np.mean(np.abs(signal[:sig_len])**2)\n",
        "\n",
        "# ============================================================\n",
        "# --- Main Generation Loop ---\n",
        "# ============================================================\n",
        "for i in tqdm(range(len(existing), N_SAMPLES), total=N_SAMPLES, initial=len(existing)):\n",
        "    iq_clean = np.zeros(num_samples, dtype=complex)\n",
        "    total_power = 0\n",
        "\n",
        "    # --- Wi-Fi ---\n",
        "    wifi_dur = np.random.uniform(0.0015, 0.0025)\n",
        "    wifi_samples = int(wifi_dur * fs)\n",
        "    wifi_start = np.random.randint(0, num_samples)\n",
        "    wifi_scale = 10 ** (np.random.uniform(-5, +5) / 20)\n",
        "    wifi_center = np.random.choice(wifi_channels)\n",
        "    wifi_sig = generate_wifi_packet(100)[:wifi_samples] * wifi_scale\n",
        "    wifi_sig = place_carrier(wifi_sig, fs, wifi_center, center_frequency)\n",
        "    total_power += safe_insert(iq_clean, wifi_sig, wifi_start)\n",
        "\n",
        "    # --- Bluetooth ---\n",
        "    cur_sample = 0\n",
        "    for _ in range(total_hop_slots):\n",
        "        f_bt_abs = np.random.choice(bt_channels)\n",
        "        f_bt_rel = f_bt_abs - center_frequency\n",
        "        if np.abs(f_bt_rel) < (fs / 2):\n",
        "            gfsk = gfsk_modulate(bt_symbols_per_hop, bt_samples_per_symbol)\n",
        "            t_hop = np.arange(hop_duration_samples) / fs\n",
        "            carrier = np.exp(1j * 2 * np.pi * f_bt_rel * t_hop)\n",
        "            bt_scale = 10 ** (np.random.uniform(-8, +5) / 20)\n",
        "            hop_signal = gfsk * carrier * bt_scale\n",
        "            total_power += safe_insert(iq_clean, hop_signal, cur_sample)\n",
        "        cur_sample += hop_duration_samples\n",
        "\n",
        "    # --- ZigBee ---\n",
        "    zb_samples = int(zigbee_dur * fs)\n",
        "    zb_start = np.random.randint(0, num_samples)\n",
        "    zb_scale = 10 ** (np.random.uniform(-10, +3) / 20)\n",
        "    zb_center = np.random.choice(zigbee_channels)\n",
        "    zb_sig = place_carrier(master_zb_packet[:zb_samples] * zb_scale, fs, zb_center, center_frequency)\n",
        "    total_power += safe_insert(iq_clean, zb_sig, zb_start)\n",
        "\n",
        "    # --- Add noise ---\n",
        "    noise_power = total_power / snr_linear\n",
        "    noise_std = np.sqrt(noise_power / 2)\n",
        "    noise = noise_std * (np.random.randn(num_samples) + 1j * np.random.randn(num_samples))\n",
        "    iq_noisy = iq_clean + noise\n",
        "\n",
        "    # --- Spectrogram ---\n",
        "    f, t, Sxx = spectrogram(iq_noisy, fs=fs, nperseg=512, noverlap=256, return_onesided=False)\n",
        "    Sxx_dB = 10 * np.log10(np.abs(Sxx) + 1e-12)\n",
        "    Sxx_dB = np.fft.fftshift(Sxx_dB, axes=0).astype(np.float32)\n",
        "\n",
        "    # --- Save ---\n",
        "    np.savez_compressed(\n",
        "        os.path.join(npz_dir, f\"wifi_bt_zigbee_{i:04d}.npz\"),\n",
        "        spectrogram_data=Sxx_dB,\n",
        "        fs=fs,\n",
        "        f_center=center_frequency,\n",
        "        label=\"wifi+bluetooth+zigbee\",\n",
        "        wifi_center=wifi_center,\n",
        "        bt_scale=bt_scale,\n",
        "        zb_center=zb_center,\n",
        "        wifi_scale=wifi_scale,\n",
        "        zb_scale=zb_scale,\n",
        "        snr_db=snr_db\n",
        "    )\n",
        "\n",
        "print(f\"Saved {N_SAMPLES} files in: {npz_dir}\")\n",
        "print(f\"Spectrograms saved permanently to: {npz_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KivosDRcAiyL",
        "outputId": "c825a2ab-839a-4718-babd-279982f377eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 300 Wi-Fi + Bluetooth spectrograms...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [08:34<00:00,  1.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved 300 files in: /content/drive/MyDrive/wifi_bluetooth_dataset/npz\n",
            "Spectrograms saved permanently to: /content/drive/MyDrive/wifi_bluetooth_dataset/npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import spectrogram, windows, firwin\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# --- CONFIGURATION ---\n",
        "# ============================================================\n",
        "fs = 20e6\n",
        "snr_db = 10\n",
        "sim_time = 0.105\n",
        "num_samples = int(sim_time * fs)\n",
        "center_frequency = 2437e6\n",
        "snr_linear = 10 ** (snr_db / 10)\n",
        "N_SAMPLES = 300\n",
        "\n",
        "# ============================================================\n",
        "# --- PATHS ---\n",
        "# ============================================================\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "dataset_dir = os.path.join(base_path, \"wifi_bluetooth_dataset\")\n",
        "npz_dir = os.path.join(dataset_dir, \"npz\")\n",
        "os.makedirs(npz_dir, exist_ok=True)\n",
        "\n",
        "existing = [f for f in os.listdir(npz_dir) if f.endswith(\".npz\")]\n",
        "if len(existing) >= N_SAMPLES:\n",
        "    print(f\"Dataset already complete with {len(existing)} files in: {npz_dir}\")\n",
        "    print(f\"Spectrograms saved permanently to: {npz_dir}\")\n",
        "    exit()\n",
        "else:\n",
        "    print(f\"Generating {N_SAMPLES - len(existing)} Wi-Fi + Bluetooth spectrograms...\")\n",
        "\n",
        "# ============================================================\n",
        "# --- BLUETOOTH PARAMETERS ---\n",
        "# ============================================================\n",
        "bt_symbol_rate = 1e6\n",
        "bt_samples_per_symbol = int(fs / bt_symbol_rate)\n",
        "hop_rate = 1600\n",
        "hop_duration_sec = 1.0 / hop_rate\n",
        "hop_duration_samples = int(hop_duration_sec * fs)\n",
        "bt_symbols_per_hop = int(hop_duration_sec * bt_symbol_rate)\n",
        "bt_channels_abs = np.arange(2402e6, 2481e6, 1e6)\n",
        "total_hop_slots = int(np.floor(num_samples / hop_duration_samples))\n",
        "\n",
        "# ============================================================\n",
        "# --- WI-FI PARAMETERS ---\n",
        "# ============================================================\n",
        "wifi_bandwidth = 10e6\n",
        "nyquist = fs / 2.0\n",
        "cutoff_norm = (wifi_bandwidth / 2.0) / nyquist\n",
        "num_taps = 101\n",
        "fir_taps = firwin(num_taps, cutoff_norm, window=\"hamming\")\n",
        "wifi_channels_abs = np.array([\n",
        "    2412e6, 2417e6, 2422e6, 2427e6, 2432e6, 2437e6,\n",
        "    2442e6, 2447e6, 2452e6, 2457e6, 2462e6\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# --- FUNCTIONS ---\n",
        "# ============================================================\n",
        "def gfsk_modulate(num_symbols, samples_per_symbol, modulation_index=0.32):\n",
        "    \"\"\"Generate baseband GFSK signal.\"\"\"\n",
        "    bits = np.random.randint(0, 2, num_symbols)\n",
        "    nrz = bits * 2 - 1\n",
        "    x_rect = np.repeat(nrz, samples_per_symbol)\n",
        "    g_len = 4 * samples_per_symbol + 1\n",
        "    std = 0.35 * samples_per_symbol\n",
        "    g = windows.gaussian(g_len, std=std)\n",
        "    g /= np.sum(g)\n",
        "    x_filt = np.convolve(x_rect, g, mode=\"same\")\n",
        "    phase_step = (np.pi * modulation_index) / samples_per_symbol\n",
        "    phase = np.cumsum(x_filt * phase_step)\n",
        "    return np.exp(1j * phase)\n",
        "\n",
        "def generate_bluetooth_signal():\n",
        "    \"\"\"Simulate Bluetooth FHSS GFSK across hops.\"\"\"\n",
        "    iq_bt = np.zeros(num_samples, dtype=complex)\n",
        "    current_sample = 0\n",
        "    for _ in range(total_hop_slots):\n",
        "        hop_freq_abs = np.random.choice(bt_channels_abs)\n",
        "        hop_freq_rel = hop_freq_abs - center_frequency\n",
        "        if np.abs(hop_freq_rel) < fs / 2:\n",
        "            gfsk = gfsk_modulate(bt_symbols_per_hop, bt_samples_per_symbol)\n",
        "            t = np.arange(hop_duration_samples) / fs\n",
        "            carrier = np.exp(1j * 2 * np.pi * hop_freq_rel * t)\n",
        "            hop_signal = gfsk * carrier\n",
        "            end_idx = min(current_sample + hop_duration_samples, num_samples)\n",
        "            iq_bt[current_sample:end_idx] = hop_signal[:end_idx - current_sample]\n",
        "        current_sample += hop_duration_samples\n",
        "    return iq_bt\n",
        "\n",
        "def generate_wifi_burst():\n",
        "    \"\"\"Simulate Wi-Fi burst using filtered wideband noise.\"\"\"\n",
        "    iq_wifi = np.zeros(num_samples, dtype=complex)\n",
        "    wifi_freq_abs = np.random.choice(wifi_channels_abs)\n",
        "    wifi_freq_rel = wifi_freq_abs - center_frequency\n",
        "    burst_duration = np.random.uniform(0.001, 0.005)\n",
        "    burst_samples = int(burst_duration * fs)\n",
        "    start_sample = np.random.randint(0, num_samples - burst_samples)\n",
        "    wideband_noise = (np.random.randn(burst_samples) + 1j * np.random.randn(burst_samples)) / np.sqrt(2)\n",
        "    filtered = np.convolve(wideband_noise, fir_taps, mode=\"same\")\n",
        "    t = np.arange(burst_samples) / fs\n",
        "    carrier = np.exp(1j * 2 * np.pi * wifi_freq_rel * t)\n",
        "    modulated = filtered * carrier\n",
        "    iq_wifi[start_sample:start_sample + burst_samples] = modulated\n",
        "    return iq_wifi, wifi_freq_abs\n",
        "\n",
        "# ============================================================\n",
        "# --- MAIN GENERATION LOOP ---\n",
        "# ============================================================\n",
        "for run_idx in tqdm(range(len(existing), N_SAMPLES), initial=len(existing), total=N_SAMPLES):\n",
        "    iq_bt = generate_bluetooth_signal()\n",
        "    iq_wifi, wifi_freq = generate_wifi_burst()\n",
        "\n",
        "    # --- Normalize & combine ---\n",
        "    bt_power = np.mean(np.abs(iq_bt)**2)\n",
        "    wifi_power = np.mean(np.abs(iq_wifi)**2)\n",
        "    iq_bt_norm = iq_bt / np.sqrt(bt_power + 1e-12)\n",
        "    iq_wifi_norm = iq_wifi / np.sqrt(wifi_power + 1e-12)\n",
        "    wifi_scale = np.random.uniform(1.2, 1.8)\n",
        "    bt_scale = np.random.uniform(0.8, 1.2)\n",
        "    iq_clean = (wifi_scale * iq_wifi_norm) + (bt_scale * iq_bt_norm)\n",
        "\n",
        "    # --- Add AWGN ---\n",
        "    sig_power = np.mean(np.abs(iq_clean)**2)\n",
        "    noise_power = sig_power / snr_linear\n",
        "    noise_std = np.sqrt(noise_power / 2)\n",
        "    noise = noise_std * (np.random.randn(num_samples) + 1j * np.random.randn(num_samples))\n",
        "    iq_noisy = iq_clean + noise\n",
        "\n",
        "    # --- Spectrogram ---\n",
        "    f, t, Sxx = spectrogram(iq_noisy, fs=fs, nperseg=1024, noverlap=512, return_onesided=False)\n",
        "    Sxx_dB = 10 * np.log10(np.abs(Sxx) + 1e-12)\n",
        "    Sxx_dB = np.fft.fftshift(Sxx_dB, axes=0).astype(np.float32)\n",
        "\n",
        "    # --- Save ---\n",
        "    np.savez_compressed(\n",
        "        os.path.join(npz_dir, f\"wifi_bluetooth_{run_idx:04d}.npz\"),\n",
        "        spectrogram_data=Sxx_dB,\n",
        "        fs=fs,\n",
        "        f_center=center_frequency,\n",
        "        wifi_freq=wifi_freq,\n",
        "        wifi_scale=wifi_scale,\n",
        "        bt_scale=bt_scale,\n",
        "        snr_db=snr_db,\n",
        "        label=\"wifi+bluetooth\"\n",
        "    )\n",
        "\n",
        "print(f\"Saved {N_SAMPLES} files in: {npz_dir}\")\n",
        "print(f\"Spectrograms saved permanently to: {npz_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32c9Xp2Q_3JA"
      },
      "source": [
        "CNN Model 1 (Majority Signal Detector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UHHCLmli_2uJ",
        "outputId": "400f1c99-273d-4308-ac60-198e84ee444e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Class weights: [0.5833333 2.3333333 1.1666666]\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.83M/9.83M [00:00<00:00, 142MB/s]\n",
            "/tmp/ipython-input-3748038366.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01: train 0.5682/0.7518 | val 0.6534/0.8476\n",
            "  ↪ Saved best model.\n",
            "Epoch 02: train 0.0458/0.9887 | val 0.8519/0.7762\n",
            "Epoch 03: train 0.0218/0.9940 | val 2.0653/0.6857\n",
            "Epoch 04: train 0.0212/0.9940 | val 1.5512/0.7286\n",
            "Epoch 05: train 0.0231/0.9940 | val 1.6645/0.7286\n",
            "Epoch 06: train 0.0113/0.9952 | val 2.0097/0.7143\n",
            "Epoch 07: train 0.0022/1.0000 | val 2.0614/0.7048\n",
            "Early stopping.\n",
            "\n",
            "Test Loss 0.6452 | Acc 0.8952\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        wifi     0.9310    0.8852    0.9076       122\n",
            "   bluetooth     1.0000    0.8800    0.9362        25\n",
            "      zigbee     0.8056    0.9206    0.8593        63\n",
            "\n",
            "    accuracy                         0.8952       210\n",
            "   macro avg     0.9122    0.8953    0.9010       210\n",
            "weighted avg     0.9016    0.8952    0.8965       210\n",
            "\n",
            "Confusion Matrix:\n",
            "[[108   0  14]\n",
            " [  3  22   0]\n",
            " [  5   0  58]]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3748038366.py:269: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(model, dummy, \"majority_mnv3s_best.onnx\",\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Exported best model:\n",
            "  - PyTorch: majority_mnv3s_best.pt\n",
            "  - ONNX:    majority_mnv3s_best.onnx\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CNN-1: Majority Signal Detector (Hackathon-Ready Robust Version)\n",
        "# ============================================================\n",
        "\n",
        "import os, glob, random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Dict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# ============================================================\n",
        "# --- PATHS ---\n",
        "# ============================================================\n",
        "DATA_ROOT = \"/content/drive/MyDrive\"\n",
        "FOLDERS = {\n",
        "    \"wifi\":              \"wifi_dataset/npz\",\n",
        "    \"bt\":                \"bluetooth_dataset/npz\",\n",
        "    \"zb\":                \"zigbee_dataset/npz\",\n",
        "    \"wifi_bt\":           \"wifi_bluetooth_dataset/npz\",\n",
        "    \"zb_bt\":             \"bluetooth_zigbee_dataset/npz\",\n",
        "    \"wifi_zb\":           \"wifi_zigbee_dataset/npz\",\n",
        "    \"wifi_bt_zb\":        \"wifi_bluetooth_zigbee_dataset/npz\",\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# --- CLASS MAP (7 → 3)\n",
        "# ============================================================\n",
        "MAJORITY_MAP = {\n",
        "    \"wifi\": 0, \"bt\": 1, \"zb\": 2,\n",
        "    \"wifi_bt\": 0, \"zb_bt\": 2, \"wifi_zb\": 0, \"wifi_bt_zb\": 0,\n",
        "}\n",
        "IDX2NAME_3 = {0: \"wifi\", 1: \"bluetooth\", 2: \"zigbee\"}\n",
        "NPZ_KEY = \"spectrogram_data\"\n",
        "\n",
        "# ============================================================\n",
        "# --- CONFIG ---\n",
        "# ============================================================\n",
        "IMG_SIZE = 160\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "LR = 3e-4\n",
        "VAL_SPLIT, TEST_SPLIT = 0.1, 0.1\n",
        "LABEL_SMOOTH = 0.15\n",
        "PATIENCE = 6\n",
        "NUM_WORKERS = 0\n",
        "WEIGHT_DECAY = 1e-3\n",
        "SEED = 1337\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# --- DATASET ---\n",
        "# ============================================================\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, root, folders, majormap, img_size=160, augment=True):\n",
        "        self.samples = []\n",
        "        self.majormap = majormap\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "\n",
        "        for key, subpath in folders.items():\n",
        "            folder_path = os.path.join(root, subpath)\n",
        "            if not os.path.exists(folder_path):\n",
        "                raise FileNotFoundError(f\"Missing folder: {folder_path}\")\n",
        "            files = glob.glob(os.path.join(folder_path, \"*.npz\"))\n",
        "            for p in files:\n",
        "                self.samples.append((p, key))\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"No .npz files found in dataset folders.\")\n",
        "\n",
        "        self.train_tf = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "            transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomErasing(p=0.4, scale=(0.02, 0.25), value=0),\n",
        "        ])\n",
        "        self.eval_tf = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, key = self.samples[idx]\n",
        "        arr = np.load(path)[NPZ_KEY].astype(np.float32)\n",
        "        arr = (arr - arr.mean()) / (arr.std() + 1e-6)\n",
        "        im = Image.fromarray(arr).convert(\"F\")\n",
        "        t = self.train_tf(im) if self.augment else self.eval_tf(im)\n",
        "        return t, self.majormap[key], key\n",
        "\n",
        "# ============================================================\n",
        "# --- SPLIT HELPERS ---\n",
        "# ============================================================\n",
        "def split_indices(n, val_ratio=0.1, test_ratio=0.1):\n",
        "    idxs = list(range(n))\n",
        "    random.shuffle(idxs)\n",
        "    n_test = int(n * test_ratio)\n",
        "    n_val = int(n * val_ratio)\n",
        "    return idxs[n_test+n_val:], idxs[n_test:n_test+n_val], idxs[:n_test]\n",
        "\n",
        "class SubsetDataset(Dataset):\n",
        "    def __init__(self, base, idxs, aug=False):\n",
        "        self.base, self.idxs, self.aug = base, idxs, aug\n",
        "    def __len__(self): return len(self.idxs)\n",
        "    def __getitem__(self, i):\n",
        "        old = self.base.augment\n",
        "        self.base.augment = self.aug\n",
        "        out = self.base[self.idxs[i]]\n",
        "        self.base.augment = old\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# --- MODEL ---\n",
        "# ============================================================\n",
        "class MobileNetV3Small3(nn.Module):\n",
        "    def __init__(self, pretrained=True, dropout=0.4):\n",
        "        super().__init__()\n",
        "        weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        m = mobilenet_v3_small(weights=weights)\n",
        "        # 1-channel input\n",
        "        first_conv = m.features[0][0]\n",
        "        new_conv = nn.Conv2d(1, first_conv.out_channels,\n",
        "                             kernel_size=first_conv.kernel_size,\n",
        "                             stride=first_conv.stride,\n",
        "                             padding=first_conv.padding,\n",
        "                             bias=False)\n",
        "        if pretrained:\n",
        "            with torch.no_grad():\n",
        "                new_conv.weight[:] = first_conv.weight.mean(dim=1, keepdim=True)\n",
        "        m.features[0][0] = new_conv\n",
        "        # The actual final feature size for mobilenet_v3_small is 576, not 1024.\n",
        "        m.classifier = nn.Sequential(\n",
        "            nn.Linear(576, 256),\n",
        "            nn.Hardswish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 3)\n",
        "        )\n",
        "\n",
        "        self.m = m\n",
        "    def forward(self, x): return self.m(x)\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, smoothing=0.15):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "    def forward(self, logits, target):\n",
        "        n_class = logits.size(1)\n",
        "        logprobs = F.log_softmax(logits, dim=1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(logprobs)\n",
        "            true_dist.fill_(self.smoothing / (n_class - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
        "        return torch.mean(torch.sum(-true_dist * logprobs, dim=1))\n",
        "\n",
        "# ============================================================\n",
        "# --- TRAIN / EVAL ---\n",
        "# ============================================================\n",
        "def train_epoch(model, loader, opt, criterion, device):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0\n",
        "    for x, y, _ in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0\n",
        "    yt, yp = [], []\n",
        "    for x, y, _ in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "        yt.extend(y.cpu().numpy()); yp.extend(pred.cpu().numpy())\n",
        "    return loss_sum/total, correct/total, np.array(yt), np.array(yp)\n",
        "\n",
        "# ============================================================\n",
        "# --- MAIN ---\n",
        "# ============================================================\n",
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    base_ds = SpectrogramDataset(DATA_ROOT, FOLDERS, MAJORITY_MAP, img_size=IMG_SIZE, augment=True)\n",
        "    n = len(base_ds)\n",
        "    tr_idx, va_idx, te_idx = split_indices(n, VAL_SPLIT, TEST_SPLIT)\n",
        "    tr_ds = SubsetDataset(base_ds, tr_idx, True)\n",
        "    va_ds = SubsetDataset(base_ds, va_idx, False)\n",
        "    te_ds = SubsetDataset(base_ds, te_idx, False)\n",
        "\n",
        "    train_loader = DataLoader(tr_ds, BATCH_SIZE, True, num_workers=NUM_WORKERS)\n",
        "    val_loader   = DataLoader(va_ds, BATCH_SIZE, False, num_workers=NUM_WORKERS)\n",
        "    test_loader  = DataLoader(te_ds, BATCH_SIZE, False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # --- Class weights (handles imbalance)\n",
        "    y_train = [base_ds.majormap[key] for _, key in base_ds.samples]\n",
        "    weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "    weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "    print(\"Class weights:\", weights.cpu().numpy())\n",
        "\n",
        "    model = MobileNetV3Small3(pretrained=True, dropout=0.4).to(device)\n",
        "    for p in model.m.features[:3].parameters():  # allow adaptation\n",
        "        p.requires_grad = False\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                            lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
        "\n",
        "    best_val, patience = float(\"inf\"), PATIENCE\n",
        "    best_path = \"majority_mnv3s_best.pt\"\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, opt, criterion, device)\n",
        "        va_loss, va_acc, _, _ = eval_epoch(model, val_loader, criterion, device)\n",
        "        sched.step()\n",
        "        print(f\"Epoch {ep:02d}: train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f}\")\n",
        "        if va_loss < best_val - 1e-4:\n",
        "            best_val = va_loss\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            patience = PATIENCE\n",
        "            print(\"  ↪ Saved best model.\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "            if patience == 0:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # --- TEST ---\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    te_loss, te_acc, yt, yp = eval_epoch(model, test_loader, criterion, device)\n",
        "    print(f\"\\nTest Loss {te_loss:.4f} | Acc {te_acc:.4f}\")\n",
        "    print(classification_report(yt, yp, target_names=[IDX2NAME_3[i] for i in range(3)], digits=4))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(yt, yp))\n",
        "\n",
        "    # --- ONNX EXPORT ---\n",
        "    !pip install -q onnx\n",
        "    dummy = torch.randn(1,1,IMG_SIZE,IMG_SIZE).to(device)\n",
        "    torch.onnx.export(model, dummy, \"majority_mnv3s_best.onnx\",\n",
        "                      input_names=[\"spectrogram\"], output_names=[\"logits\"],\n",
        "                      opset_version=12, do_constant_folding=True)\n",
        "    print(\"\\nExported best model:\")\n",
        "    print(\"  - PyTorch: majority_mnv3s_best.pt\")\n",
        "    print(\"  - ONNX:    majority_mnv3s_best.onnx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jshfQrDerJIy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x7cbhBuJrLkR",
        "outputId": "2b143735-4dca-4c30-ba59-98b50737c3e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-NTt-cW_otZ"
      },
      "source": [
        "CNN Model 2 (Interference Detector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RgzztFpoZR-E"
      },
      "outputs": [],
      "source": [
        "NUM_WORKERS = 0\n",
        "pin_memory = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADAYXbm2SLB2",
        "outputId": "10bcfb14-f406-41dd-db88-882203c6b9e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-367915219.py, line 183)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-367915219.py\"\u001b[0;36m, line \u001b[0;32m183\u001b[0m\n\u001b[0;31m    if triples[0][1] <= 1e-9:i5`\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CNN-2: Interference Detector (7 folders → 4 classes), OOD-hardened\n",
        "# - Robust aug: SpecAugment (time/freq mask), RR-Crop, jitter, noise\n",
        "# - MixUp, label smoothing, weight decay, grad clip, AMP\n",
        "# - Class-balanced sampler\n",
        "# - Cosine LR w/ warmup\n",
        "# - Dynamic classification_report labels\n",
        "# ============================================================\n",
        "\n",
        "import os, glob, random, math\n",
        "from typing import Dict, List, Tuple\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "DATA_ROOT = \"/content/drive/MyDrive\"\n",
        "FOLDERS = {\n",
        "    \"wifi\":              \"wifi_dataset/npz\",\n",
        "    \"bt\":                \"bluetooth_dataset/npz\",\n",
        "    \"zb\":                \"zigbee_dataset/npz\",\n",
        "    \"wifi_bt\":           \"wifi_bluetooth_dataset/npz\",\n",
        "    \"zb_bt\":             \"bluetooth_zigbee_dataset/npz\",\n",
        "    \"wifi_zb\":           \"wifi_zigbee_dataset/npz\",\n",
        "    \"wifi_bt_zb\":        \"wifi_bluetooth_zigbee_dataset/npz\",\n",
        "}\n",
        "\n",
        "INTERF_IDX = {\"none\": 0, \"wifi\": 1, \"bt\": 2, \"zb\": 3}\n",
        "\n",
        "# Policy mapping (leave as-is if you’re not using metadata)\n",
        "INTERF_MAP_POLICY = {\n",
        "    \"wifi\": \"none\",\n",
        "    \"bt\": \"none\",\n",
        "    \"zb\": \"none\",\n",
        "    \"wifi_bt\": \"bt\",\n",
        "    \"zb_bt\": \"bt\",\n",
        "    \"wifi_zb\": \"zb\",\n",
        "    \"wifi_bt_zb\": \"bt\",\n",
        "}\n",
        "\n",
        "# If your .npz has *_scale metadata, set True to compute interference = 2nd strongest\n",
        "USE_METADATA_FOR_LABELS = False\n",
        "\n",
        "# Train config\n",
        "IMG_SIZE = 160\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25\n",
        "BASE_LR = 3e-4\n",
        "WARMUP_EPOCHS = 2\n",
        "WEIGHT_DECAY = 1e-3\n",
        "VAL_SPLIT, TEST_SPLIT = 0.1, 0.1\n",
        "LABEL_SMOOTH = 0.1\n",
        "PATIENCE = 5\n",
        "NUM_WORKERS = 0\n",
        "SEED = 1337\n",
        "APPLY_NONE_THRESHOLD = True\n",
        "NONE_THRESH = 0.45\n",
        "NPZ_KEY = \"spectrogram_data\"\n",
        "GRAD_CLIP_NORM = 1.0\n",
        "MIXUP_ALPHA = 0.2    # set 0.0 to disable\n",
        "USE_BALANCED_SAMPLER = True\n",
        "pin_memory = False\n",
        "\n",
        "# Aug toggles\n",
        "AUG_RANDOM_RESIZED_CROP = True\n",
        "AUG_FREQ_MASKS = 2          # SpecAugment: number of freq masks\n",
        "AUG_TIME_MASKS = 2          # SpecAugment: number of time masks\n",
        "AUG_FREQ_MASK_PCT = 0.12\n",
        "AUG_TIME_MASK_PCT = 0.12\n",
        "AUG_JITTER_STD = 0.02       # add small gaussian noise to image\n",
        "\n",
        "# ---------------- Seed ----------------\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ---------------- Aug helpers ----------------\n",
        "def specaugment_np(arr: np.ndarray,\n",
        "                   n_freq=2, n_time=2,\n",
        "                   freq_mask_pct=0.12, time_mask_pct=0.12) -> np.ndarray:\n",
        "    \"\"\"Apply SpecAugment masks on a HxW spectrogram (float32).\"\"\"\n",
        "    H, W = arr.shape\n",
        "    a = arr.copy()\n",
        "    # Frequency masks\n",
        "    for _ in range(n_freq):\n",
        "        width = max(1, int(freq_mask_pct * H))\n",
        "        f0 = np.random.randint(0, max(1, H - width + 1))\n",
        "        a[f0:f0+width, :] = 0.0\n",
        "    # Time masks\n",
        "    for _ in range(n_time):\n",
        "        width = max(1, int(time_mask_pct * W))\n",
        "        t0 = np.random.randint(0, max(1, W - width + 1))\n",
        "        a[:, t0:t0+width] = 0.0\n",
        "    return a\n",
        "\n",
        "def random_resized_crop_np(arr: np.ndarray, out_h: int, out_w: int,\n",
        "                           scale=(0.8, 1.0), ratio=(0.9, 1.1)) -> np.ndarray:\n",
        "    H, W = arr.shape\n",
        "    area = H * W\n",
        "    for _ in range(10):\n",
        "        target_area = area * np.random.uniform(*scale)\n",
        "        log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n",
        "        aspect = math.exp(np.random.uniform(*log_ratio))\n",
        "        h = int(round(math.sqrt(target_area * aspect)))\n",
        "        w = int(round(math.sqrt(target_area / aspect)))\n",
        "        if 0 < h <= H and 0 < w <= W:\n",
        "            i = np.random.randint(0, H - h + 1)\n",
        "            j = np.random.randint(0, W - w + 1)\n",
        "            crop = arr[i:i+h, j:j+w]\n",
        "            return np.array(Image.fromarray(crop).resize((out_w, out_h), Image.BILINEAR))\n",
        "    # Fallback: just resize\n",
        "    return np.array(Image.fromarray(arr).resize((out_w, out_h), Image.BILINEAR))\n",
        "\n",
        "# ---------------- Dataset ----------------\n",
        "class InterfDataset(Dataset):\n",
        "    def __init__(self, root: str, folders: Dict[str, str],\n",
        "                 interf_map_policy: Dict[str, str],\n",
        "                 img_size=160, augment=True):\n",
        "        self.samples: List[Tuple[str, str]] = []\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.interf_map_policy = interf_map_policy\n",
        "\n",
        "        for key, subpath in folders.items():\n",
        "            folder = os.path.join(root, subpath)\n",
        "            if not os.path.isdir(folder):\n",
        "                raise FileNotFoundError(f\"Missing folder: {folder}\")\n",
        "            files = glob.glob(os.path.join(folder, \"*.npz\"))\n",
        "            for p in files:\n",
        "                self.samples.append((p, key))\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(\"No .npz files found.\")\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def _np_to_tensor(self, arr: np.ndarray, train=True) -> torch.Tensor:\n",
        "        arr = np.asarray(arr, dtype=np.float32)\n",
        "        # Per-sample z-norm (robust to absolute dB ranges)\n",
        "        m, s = arr.mean(), arr.std()\n",
        "        if s < 1e-6: s = 1.0\n",
        "        arr = (arr - m) / s\n",
        "\n",
        "        # Random resized crop (keep global patterns but vary scale/position)\n",
        "        if train and AUG_RANDOM_RESIZED_CROP:\n",
        "            arr = random_resized_crop_np(arr, IMG_SIZE, IMG_SIZE, scale=(0.8, 1.0), ratio=(0.9, 1.1))\n",
        "        else:\n",
        "            arr = np.array(Image.fromarray(arr).resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR))\n",
        "\n",
        "        # SpecAugment masks\n",
        "        if train and (AUG_FREQ_MASKS > 0 or AUG_TIME_MASKS > 0):\n",
        "            arr = specaugment_np(arr, AUG_FREQ_MASKS, AUG_TIME_MASKS, AUG_FREQ_MASK_PCT, AUG_TIME_MASK_PCT)\n",
        "\n",
        "        # Small Gaussian jitter\n",
        "        if train and AUG_JITTER_STD > 0:\n",
        "            arr = arr + np.random.randn(*arr.shape).astype(np.float32) * AUG_JITTER_STD\n",
        "\n",
        "        # Occasional horizontal flip (time reversal)\n",
        "        if train and random.random() < 0.5:\n",
        "            arr = np.ascontiguousarray(arr[:, ::-1])\n",
        "\n",
        "        t = torch.from_numpy(arr).unsqueeze(0)  # [1,H,W]\n",
        "        return t\n",
        "\n",
        "    def _infer_label_from_policy(self, seven_key: str) -> int:\n",
        "        return INTERF_IDX[self.interf_map_policy[seven_key]]\n",
        "\n",
        "    def _infer_label_from_metadata(self, z, seven_key: str) -> int:\n",
        "        w = float(z.get(\"wifi_scale\", 0.0))\n",
        "        b = float(z.get(\"bt_scale\", z.get(\"bluetooth_scale\", 0.0)))\n",
        "        zb = float(z.get(\"zb_scale\", z.get(\"zigbee_scale\", 0.0)))\n",
        "        triples = [(\"wifi\", abs(w)), (\"bt\", abs(b)), (\"zb\", abs(zb))]\n",
        "        triples.sort(key=lambda kv: kv[1], reverse=True)\n",
        "        if triples[0][1] <= 1e-9:i5`\n",
        "            return INTERF_IDX[\"none\"]\n",
        "        return INTERF_IDX[triples[1][0]]  # second-strongest\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        path, key = self.samples[idx]\n",
        "        z = np.load(path, allow_pickle=True)\n",
        "        arr = z[NPZ_KEY]\n",
        "        x = self._np_to_tensor(arr, train=self.augment)\n",
        "        if USE_METADATA_FOR_LABELS:\n",
        "            try:\n",
        "                y = self._infer_label_from_metadata(z, key)\n",
        "            except Exception:\n",
        "                y = self._infer_label_from_policy(key)\n",
        "        else:\n",
        "            y = self._infer_label_from_policy(key)\n",
        "        return x, int(y), key\n",
        "\n",
        "# ---------------- Split + Sampler ----------------\n",
        "def split_indices(n, val_ratio=0.1, test_ratio=0.1):\n",
        "    idxs = list(range(n))\n",
        "    random.shuffle(idxs)\n",
        "    n_test = int(n * test_ratio)\n",
        "    n_val = int(n * val_ratio)\n",
        "    return idxs[n_test+n_val:], idxs[n_test:n_test+n_val], idxs[:n_test]\n",
        "\n",
        "class SubsetDataset(Dataset):\n",
        "    def __init__(self, base: Dataset, idxs: List[int], aug: bool):\n",
        "        self.base, self.idxs, self.aug = base, idxs, aug\n",
        "    def __len__(self): return len(self.idxs)\n",
        "    def __getitem__(self, i):\n",
        "        old = self.base.augment\n",
        "        self.base.augment = self.aug\n",
        "        out = self.base[self.idxs[i]]\n",
        "        self.base.augment = old\n",
        "        return out\n",
        "\n",
        "def make_weights_for_balanced_sampler(ds: Dataset, idxs: List[int]) -> List[float]:\n",
        "    counts = [0,0,0,0]\n",
        "    labels = []\n",
        "    for i in idxs:\n",
        "        _, y, _ = ds[i]\n",
        "        labels.append(y); counts[y] += 1\n",
        "    counts = [max(c,1) for c in counts]\n",
        "    weights = [1.0 / counts[y] for y in labels]\n",
        "    return weights\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "class MobileNetV3Small4(nn.Module):\n",
        "    def __init__(self, pretrained=True, dropout_p=0.2):\n",
        "        super().__init__()\n",
        "        weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        m = mobilenet_v3_small(weights=weights)\n",
        "        # adapt first conv to 1ch\n",
        "        first_conv = m.features[0][0]\n",
        "        new_conv = nn.Conv2d(1, first_conv.out_channels,\n",
        "                             kernel_size=first_conv.kernel_size,\n",
        "                             stride=first_conv.stride,\n",
        "                             padding=first_conv.padding, bias=False)\n",
        "        with torch.no_grad():\n",
        "            if pretrained:\n",
        "                new_conv.weight[:] = first_conv.weight.mean(1, keepdim=True)\n",
        "        m.features[0][0] = new_conv\n",
        "        in_f = m.classifier[-1].in_features\n",
        "        # add stronger dropout before head\n",
        "        m.classifier[-1] = nn.Identity()\n",
        "        self.m = m\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(in_f, 4)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        feats = self.m(x)\n",
        "        return self.head(feats)\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "    def forward(self, logits, target):\n",
        "        n_class = logits.size(1)\n",
        "        logprobs = F.log_softmax(logits, dim=1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(logprobs)\n",
        "            true_dist.fill_(self.smoothing / (n_class - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
        "        return torch.mean(torch.sum(-true_dist * logprobs, dim=1))\n",
        "\n",
        "def apply_none_threshold(logits: torch.Tensor, thr: float):\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    maxp, pred = probs.max(1)\n",
        "    pred = pred.clone()\n",
        "    pred[maxp < thr] = 0\n",
        "    return pred\n",
        "\n",
        "# ---------------- MixUp ----------------\n",
        "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
        "    if alpha <= 0.0:  # disabled\n",
        "        return x, y, 1.0, y\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    bsz = x.size(0)\n",
        "    index = torch.randperm(bsz, device=x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, lam, y_b\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# ---------------- Train / Eval ----------------\n",
        "def cosine_lr_with_warmup(optimizer, epoch, base_lr, total_epochs, warmup_epochs):\n",
        "    if epoch < warmup_epochs:\n",
        "        lr = base_lr * float(epoch + 1) / float(max(1, warmup_epochs))\n",
        "    else:\n",
        "        progress = float(epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
        "        lr = 0.5 * base_lr * (1 + math.cos(math.pi * progress))\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, device, scaler: torch.cuda.amp.GradScaler):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for x, y, _ in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "        # MixUp\n",
        "        x_mix, y_a, lam, y_b = mixup_data(x, y, MIXUP_ALPHA)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            logits = model(x_mix)\n",
        "            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        # compute mixup accuracy proxy (argmax vs y)\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(1)\n",
        "            correct += ((pred == y).float().mean().item() * x.size(0))  # proxy\n",
        "        total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, criterion, device, apply_thresh=False, thr=0.45):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    yt, yp, keys = [], [], []\n",
        "    for x, y, k in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        pred = apply_none_threshold(logits, thr) if apply_thresh else logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "        yt.extend(y.cpu().numpy()); yp.extend(pred.cpu().numpy()); keys.extend(k)\n",
        "    return loss_sum/total, correct/total, np.array(yt), np.array(yp), keys\n",
        "\n",
        "def make_7x4_contingency(keys: List[str], preds: np.ndarray):\n",
        "    seven = [\"wifi\",\"bt\",\"zb\",\"wifi_bt\",\"zb_bt\",\"wifi_zb\",\"wifi_bt_zb\"]\n",
        "    table = {k: [0,0,0,0] for k in seven}\n",
        "    for k,p in zip(keys, preds):\n",
        "        if k in table and 0 <= p < 4:\n",
        "            table[k][p] += 1\n",
        "    return table\n",
        "\n",
        "def print_contingency(table):\n",
        "    cols = [\"pred_none\",\"pred_wifi\",\"pred_bt\",\"pred_zb\"]\n",
        "    print(\"\\nPer-input-class 7x4 contingency (rows=true, cols=pred):\")\n",
        "    print(\"seven_class\".ljust(18) + \"\".join([c.rjust(12) for c in cols]))\n",
        "    for k,row in table.items():\n",
        "        print(k.ljust(18) + \"\".join([str(v).rjust(12) for v in row]))\n",
        "\n",
        "# ---------------- Main ----------------\n",
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    base_ds = InterfDataset(DATA_ROOT, FOLDERS, INTERF_MAP_POLICY, img_size=IMG_SIZE, augment=True)\n",
        "    n = len(base_ds)\n",
        "    tr_idx, va_idx, te_idx = split_indices(n, VAL_SPLIT, TEST_SPLIT)\n",
        "    tr_ds = SubsetDataset(base_ds, tr_idx, True)\n",
        "    va_ds = SubsetDataset(base_ds, va_idx, False)\n",
        "    te_ds = SubsetDataset(base_ds, te_idx, False)\n",
        "\n",
        "    # Balanced sampler for TRAIN\n",
        "    if USE_BALANCED_SAMPLER:\n",
        "        weights = make_weights_for_balanced_sampler(base_ds, tr_idx)\n",
        "        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
        "        train_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
        "                                  num_workers=NUM_WORKERS, pin_memory=pin_memory, drop_last=True)\n",
        "    else:\n",
        "        train_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                  num_workers=NUM_WORKERS, pin_memory=pin_memory, drop_last=True)\n",
        "\n",
        "    val_loader = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                            num_workers=NUM_WORKERS, pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                            num_workers=NUM_WORKERS, pin_memory=pin_memory)\n",
        "\n",
        "    model = MobileNetV3Small4(pretrained=True, dropout_p=0.25).to(device)\n",
        "    # Freeze earlier layers a bit, fine-tune later ones\n",
        "    for p in model.m.features[:5].parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    criterion = LabelSmoothingCE(LABEL_SMOOTH).to(device)\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                                  lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
        "\n",
        "    best_val, patience = float(\"inf\"), PATIENCE\n",
        "    best_path = \"interf_mnv3s_best.pt\"\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        lr_now = cosine_lr_with_warmup(optimizer, ep-1, BASE_LR, EPOCHS, WARMUP_EPOCHS)\n",
        "\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
        "        va_loss, va_acc, _, _, _ = eval_epoch(model, val_loader, criterion, device,\n",
        "                                              apply_thresh=APPLY_NONE_THRESHOLD, thr=NONE_THRESH)\n",
        "\n",
        "        print(f\"Epoch {ep:02d} | lr {lr_now:.2e} | train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f}\")\n",
        "\n",
        "        if va_loss < best_val - 1e-4:\n",
        "            best_val = va_loss\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            patience = PATIENCE\n",
        "            print(\"  ↪ saved best\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "            if patience == 0:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # --- TEST ---\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    te_loss, te_acc, yt, yp, kt = eval_epoch(model, test_loader, criterion, device,\n",
        "                                             apply_thresh=APPLY_NONE_THRESHOLD, thr=NONE_THRESH)\n",
        "    print(f\"\\nTest Loss {te_loss:.4f} | Acc {te_acc:.4f}\")\n",
        "\n",
        "    unique_labels = sorted(np.unique(np.concatenate([yt, yp])))\n",
        "    idx2name = {v:k for k,v in INTERF_IDX.items()}\n",
        "    label_names = [idx2name[i] for i in unique_labels]\n",
        "    print(\"\\nClassification report (present classes only):\")\n",
        "    print(classification_report(yt, yp, labels=unique_labels, target_names=label_names, digits=4))\n",
        "\n",
        "    cm = confusion_matrix(yt, yp, labels=[0,1,2,3])\n",
        "    print(\"\\nConfusion Matrix (4x4) [rows=true, cols=pred]:\")\n",
        "    print(cm)\n",
        "\n",
        "    cont = make_7x4_contingency(kt, yp)\n",
        "    print_contingency(cont)\n",
        "\n",
        "    # --- ONNX ---\n",
        "    try:\n",
        "        import onnx  # noqa: F401\n",
        "        dummy = torch.randn(1,1,IMG_SIZE,IMG_SIZE).to(device)\n",
        "        torch.onnx.export(model, dummy, \"interf_mnv3s_best.onnx\",\n",
        "                          input_names=[\"spectrogram\"], output_names=[\"logits\"],\n",
        "                          opset_version=12, do_constant_folding=True)\n",
        "        print(\"\\nExported ONNX: interf_mnv3s_best.onnx\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n(ONNX export skipped: {e})\")\n",
        "    print(\"Saved PyTorch: interf_mnv3s_best.pt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}